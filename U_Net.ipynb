{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch implementation of U-Net: https://arxiv.org/pdf/1505.04597.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-FGSWGKxDvF"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjxg3Os3iwIr"
      },
      "source": [
        "Fetching urls of training images and labels from github. https://github.com/anuraglamsal/Nepali-Currency-Images-and-Segmentation-Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxv0QdgcivpC"
      },
      "outputs": [],
      "source": [
        "def get_github_folder_contents(owner, repo, path):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(\"Failed to fetch folder contents.\")\n",
        "        return None\n",
        "\n",
        "def get_images(owner, repo, path):\n",
        "    contents = get_github_folder_contents(owner, repo, path)\n",
        "    images = [item['download_url'] for item in contents]\n",
        "    return images\n",
        "\n",
        "owner = \"anuraglamsal\"\n",
        "repo = \"training_images\"\n",
        "path_1 = \"images\"\n",
        "path_2 = \"labels\"\n",
        "\n",
        "images, labels = shuffle(get_images(owner, repo, path_1), get_images(owner, repo, path_2), random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UybW0OcNj2_8"
      },
      "source": [
        "Writing the dataset class and doing train, validation and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ugZhO_tj5CD"
      },
      "outputs": [],
      "source": [
        "class CurrencyDataset(Dataset):\n",
        "    def __init__(self, label_urls, image_urls, transform=None, target_transform=None):\n",
        "        self.label_urls = label_urls\n",
        "        self.image_urls = image_urls\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_urls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(requests.get(self.image_urls[idx], stream=True).raw)\n",
        "        label = Image.open(requests.get(self.label_urls[idx], stream=True).raw)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "split_idx_train = int(0.8 * len(labels))\n",
        "split_idx_test = split_idx_train + int(0.1 * len(labels))\n",
        "\n",
        "# transformations to do to the images\n",
        "transform = transforms.Compose([transforms.Resize((512, 512)), transforms.Grayscale(), transforms.ToTensor()])\n",
        "\n",
        "dataset_train = CurrencyDataset(labels[0:split_idx_train], images[0:split_idx_train], transform, transform)\n",
        "dataset_test = CurrencyDataset(labels[split_idx_train:split_idx_test], images[split_idx_train:split_idx_test], transform, transform)\n",
        "dataset_val = CurrencyDataset(labels[split_idx_test:], images[split_idx_train:], transform, transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdXTn25D4Bwo"
      },
      "source": [
        "* init_channel = number of channels in your input image.\n",
        "* next_channel = the number of channels that the result of the first convolution should have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdXOLEfw2OiF"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "  def __init__(self, init_channel, next_channel):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(init_channel, next_channel, 3, padding='same')\n",
        "    self.ReLU = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(next_channel, next_channel, 3, padding='same')\n",
        "    self.maxPool = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
        "    self.conv3 = nn.Conv2d(next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv4 = nn.Conv2d(2 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv5 = nn.Conv2d(2 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv6 = nn.Conv2d(4 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv7 = nn.Conv2d(4 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv8 = nn.Conv2d(8 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv9 = nn.Conv2d(8 * next_channel, 16 * next_channel, 3, padding='same')\n",
        "    self.conv10 = nn.Conv2d(16 * next_channel, 16 * next_channel, 3, padding='same')\n",
        "    self.upsample = nn.Upsample(scale_factor=2)\n",
        "    self.conv11 = nn.Conv2d(16 * next_channel, 8 * next_channel, 2, padding='same')\n",
        "\n",
        "    #Apparently convtranspose2d also more or less does the same as upsample+conv2d.\n",
        "    #Would be interesting to explore this too perhaps. But the paper says\n",
        "    #upsample+conv, so I'll stick to this for now. Also, apparently, using transposed\n",
        "    #convolution could have \"checkboard artifacts\" as compared to upsample+conv:\n",
        "    #https://distill.pub/2016/deconv-checkerboard/\n",
        "\n",
        "    #self.conv11 = nn.ConvTranspose2d(16 * next_channel, 8 * next_channel, 2, 2)\n",
        "\n",
        "    self.conv12 = nn.Conv2d(16 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv13 = nn.Conv2d(8 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv14 = nn.Conv2d(8 * next_channel, 4 * next_channel, 2, padding='same')\n",
        "    self.conv15 = nn.Conv2d(8 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv16 = nn.Conv2d(4 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv17 = nn.Conv2d(4 * next_channel, 2 * next_channel, 2, padding='same')\n",
        "    self.conv18 = nn.Conv2d(4 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv19 = nn.Conv2d(2 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv20 = nn.Conv2d(2 * next_channel, next_channel, 2, padding='same')\n",
        "    self.conv21 = nn.Conv2d(2 * next_channel, next_channel, 3, padding='same')\n",
        "    self.conv22 = nn.Conv2d(next_channel, next_channel, 3, padding='same')\n",
        "    self.conv23 = nn.Conv2d(next_channel, 1, 1)\n",
        "\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    # self.initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    block_1 = self.ReLU(self.conv2(self.ReLU(self.conv1(x))))\n",
        "\n",
        "    block_2 = self.ReLU(self.conv4(self.ReLU(self.conv3(self.maxPool(block_1)))))\n",
        "\n",
        "    block_3 = self.ReLU(self.conv6(self.ReLU(self.conv5(self.maxPool(block_2)))))\n",
        "\n",
        "    block_4 = self.ReLU(self.conv8(self.ReLU(self.conv7(self.maxPool(block_3)))))\n",
        "    #print(block_4.shape[2])\n",
        "\n",
        "    block_5 = self.ReLU(self.conv10(self.ReLU(self.conv9(self.maxPool(block_4))))) # bottom-most block\n",
        "    #print(block_5.shape[2])\n",
        "\n",
        "    up_conv_1 = self.conv11(self.upsample(block_5))\n",
        "    crop_1 = transforms.CenterCrop(up_conv_1.shape[2])\n",
        "    block_6 = self.ReLU(self.conv13(self.ReLU(self.conv12(torch.cat((crop_1(block_4), up_conv_1), 1)))))\n",
        "    #print(block_6.shape[2])\n",
        "\n",
        "    up_conv_2 = self.conv14(self.upsample(block_6))\n",
        "    crop_2 = transforms.CenterCrop(up_conv_2.shape[2])\n",
        "    block_7 = self.ReLU(self.conv16(self.ReLU(self.conv15(torch.cat((crop_2(block_3), up_conv_2), 1)))))\n",
        "    #print(block_7.shape[2])\n",
        "\n",
        "    up_conv_3 = self.conv17(self.upsample(block_7))\n",
        "    crop_3 = transforms.CenterCrop(up_conv_3.shape[2])\n",
        "    block_8 = self.ReLU(self.conv19(self.ReLU(self.conv18(torch.cat((crop_3(block_2), up_conv_3), 1)))))\n",
        "    #print(block_8.shape[2])\n",
        "\n",
        "    up_conv_4 = self.conv20(self.upsample(block_8))\n",
        "    crop_4 = transforms.CenterCrop(up_conv_4.shape[2])\n",
        "    block_9 = self.conv23(self.ReLU(self.conv22(self.ReLU(self.conv21(torch.cat((crop_4(block_1), up_conv_4), 1))))))\n",
        "    #print(block_9.shape[2])\n",
        "\n",
        "    output = self.activation(block_9)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XuUPweJPysq"
      },
      "source": [
        "Weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRhuWsAHlnyC"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Apply Kaiming normal initialization\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    # Initialize the biases to zeros\n",
        "                    nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdVdCBgICAJM"
      },
      "source": [
        "Wandb init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "z5SbKpN_B_fd",
        "outputId": "725a7809-6c6c-462b-b882-7dfad822ab9f"
      },
      "outputs": [],
      "source": [
        "wandb.init( # for new runs\n",
        "    project=\"Currency Segmentation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hufQVCKfg0Cb"
      },
      "source": [
        "Training staging area. Can play with things here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zImZeW1RgzAb"
      },
      "outputs": [],
      "source": [
        "model = UNet(1, 64).to(torch.device('cuda'))\n",
        "loss_fn = nn.BCELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.99)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, min_lr=1e-6)\n",
        "\n",
        "initialize_weights(model)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=dataset_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=dataset_val,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=dataset_test,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "model_path = \"/path/to/model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZlTLGQzygV"
      },
      "source": [
        "Testing and validation code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDGqML24LWxG"
      },
      "outputs": [],
      "source": [
        "def test(loader, model):\n",
        "  model.eval()\n",
        "\n",
        "  total_ones_target = 0\n",
        "  total_ones_intersection = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for data, target in loader:\n",
        "      data, target = data.to(torch.device('cuda')), target.to(torch.device('cuda'))\n",
        "\n",
        "      output = model(data)\n",
        "\n",
        "      pred = (output > 0.5).float() # binarizing prediction\n",
        "      target = (target > 0.5).float() # binarizing actual\n",
        "\n",
        "      intersection = pred * target\n",
        "\n",
        "      total_ones_target += target.sum().item()\n",
        "      total_ones_intersection += intersection.sum().item()\n",
        "\n",
        "  accuracy = 100.0 * total_ones_intersection / total_ones_target\n",
        "\n",
        "  print(f\"Average IOU basically: {accuracy}\")\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NNdghybm8oN"
      },
      "source": [
        "Training code.\n",
        "\n",
        "Apparently padded convolutions is fine. The reason to not do it basically because of heavy computation. Look here: https://stackoverflow.com/questions/44014534/why-could-u-net-mask-image-with-smaller-mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsiUVNGDm8O2",
        "outputId": "dc381f73-c28d-404a-87b4-7aec890d586e"
      },
      "outputs": [],
      "source": [
        "_epoch = -1\n",
        "\n",
        "try:\n",
        "  checkpoint = torch.load(model_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  _epoch = checkpoint['epoch']\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.99)\n",
        "\n",
        "for epoch in range(_epoch+1, 100):\n",
        "  model.train()  # Set the model to training mode\n",
        "\n",
        "  #print(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "  total_epoch_loss = 0.0\n",
        "  total_num_of_pixels = 0\n",
        "\n",
        "  #print('\\n')\n",
        "\n",
        "  for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "     inputs, labels = inputs.to(torch.device('cuda')), labels.to(torch.device('cuda'))\n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     # Forward pass\n",
        "     outputs = model(inputs)\n",
        "     total_num_of_pixels += outputs.numel()\n",
        "\n",
        "     # Compute the loss\n",
        "     loss = loss_fn(outputs, labels)\n",
        "     total_epoch_loss += loss.item()\n",
        "\n",
        "     # Backward pass\n",
        "     loss.backward()\n",
        "\n",
        "     nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "     # Update the weights\n",
        "     optimizer.step()\n",
        "\n",
        "     #scheduler.step()\n",
        "     if batch_idx % 100 == 0:\n",
        "         print(total_epoch_loss / total_num_of_pixels)\n",
        "\n",
        "  #print('\\n')\n",
        "\n",
        "  avg_loss_per_pixel_train =  total_epoch_loss / total_num_of_pixels\n",
        "  acc = test(val_loader, model)\n",
        "\n",
        "  scheduler.step(int(acc)) # can play with this too i guess.\n",
        "\n",
        "  wandb.log({\"avg_loss_per_pixel_train\": avg_loss_per_pixel_train,\n",
        "               \"avg_IOU\": acc})\n",
        "\n",
        "  torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, model_path)\n",
        "\n",
        "print('\\n')\n",
        "print('Test set metrics: ')\n",
        "_ = test(test_loader, model)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8k0_LrMGgX_"
      },
      "source": [
        "Rough code to understand cross-entropy loss in torch. Btw, cross-entropy in torch does log-softmax by itself, so you can send the \"raw\" output directly. The log used is natural log. Log of base 2 could also be used, but shouldn't really matter: https://stats.stackexchange.com/questions/295174/difference-in-log-base-for-cross-entropy-calcuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDY3dCMUfuVV",
        "outputId": "4c948a55-d59b-470b-8314-2945aed9b1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 2, 2]) torch.Size([1, 2, 2, 2])\n",
            "tensor([[[0.6163, 0.6027],\n",
            "         [0.5130, 0.7236]]])\n",
            "tensor([[[0.6163, 0.6027],\n",
            "         [0.5130, 0.7236]]])\n",
            "tensor(2.4556)\n",
            "tensor(2.4556)\n"
          ]
        }
      ],
      "source": [
        "#predicted output\n",
        "x = torch.tensor([[[0.41, 0.56], [0.69, 0.84]], [[0.57, 0.37], [0.29, 0.90]]])\n",
        "x = x.unsqueeze(0) #The generic way to store image tensors is with a batch size too.\n",
        "\n",
        "#You can either give the actual probabilities.\n",
        "y = torch.tensor([[[0., 1.], [1., 1.]],[[1., 0.], [0., 0.]]]) #target output\n",
        "y = y.unsqueeze(0) #Same reason as above.\n",
        "\n",
        "#Or you can specify which class the pixels belong to. The first class has\n",
        "#index 0, the second class has index 1, and so on. This is better computationally.\n",
        "#If you are 100% sure in your target output of the classes of the pixel i.e. you use\n",
        "#0 or 1, then it is better to use this.\n",
        "y_class_index = torch.tensor([[1, 0], [0, 0]]) #target output\n",
        "y_class_index = y_class_index.unsqueeze(0) #interpreted as [batch, height, width]\n",
        "\n",
        "#reduction 'none' gives you the cross entropy value of each pixel.\n",
        "entropy_1 = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "#reduction 'sum' gives you the cross entropy value of the whole image.\n",
        "entropy_2 = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "#The outputs are the same, but the \"class_index\" approach is computationally\n",
        "#better. Look at the \"NOTE\" portion here:\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#It makes sense that it is computationally better too. If the target corresponding\n",
        "#to an output is 0, then that output doesn't contribute to the cross-entropy\n",
        "#calculation (look at the formula for cross-entropy.) Therefore, you can\n",
        "#ignore those. With the class index approach, you are essentially telling\n",
        "#pytorch that your intention is to ignore the ignorable.\n",
        "print(entropy_1(x, y))\n",
        "print(entropy_1(x, y_class_index))\n",
        "\n",
        "print(entropy_2(x, y))\n",
        "print(entropy_2(x, y_class_index))\n",
        "\n",
        "#Interesting that stuff in pytorch is so nicely structured to be straightforward\n",
        "#for images. But maybe that's a bias because I'm only doing stuff for images.\n",
        "#Maybe stuff is straightforward for other stuff too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3ufr_HNICJ"
      },
      "source": [
        "Rough code to understand how softmax works in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBH9FtbhNHl5",
        "outputId": "371d680f-1617-4eab-aac1-6ee47dd522f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0.4452, 0.3417],\n",
            "          [0.5520, 0.3510]],\n",
            "\n",
            "         [[0.4771, 0.5547],\n",
            "          [0.5214, 0.5568]]],\n",
            "\n",
            "\n",
            "        [[[0.5548, 0.6583],\n",
            "          [0.4480, 0.6490]],\n",
            "\n",
            "         [[0.5229, 0.4453],\n",
            "          [0.4786, 0.4432]]]])\n",
            "tensor([[[[0.4998, 0.3139],\n",
            "          [0.5768, 0.4049]],\n",
            "\n",
            "         [[0.5002, 0.6861],\n",
            "          [0.4232, 0.5951]]],\n",
            "\n",
            "\n",
            "        [[[0.5319, 0.5233],\n",
            "          [0.5465, 0.6124]],\n",
            "\n",
            "         [[0.4681, 0.4767],\n",
            "          [0.4535, 0.3876]]]])\n",
            "tensor([[[[0.4083, 0.4500],\n",
            "          [0.5917, 0.5500]],\n",
            "\n",
            "         [[0.4848, 0.5488],\n",
            "          [0.5152, 0.4512]]],\n",
            "\n",
            "\n",
            "        [[[0.5144, 0.4601],\n",
            "          [0.4856, 0.5399]],\n",
            "\n",
            "         [[0.5291, 0.5509],\n",
            "          [0.4709, 0.4491]]]])\n",
            "tensor([[[[0.4966, 0.5034],\n",
            "          [0.5391, 0.4609]],\n",
            "\n",
            "         [[0.3112, 0.6888],\n",
            "          [0.3687, 0.6313]]],\n",
            "\n",
            "\n",
            "        [[[0.3896, 0.6104],\n",
            "          [0.3393, 0.6607]],\n",
            "\n",
            "         [[0.3814, 0.6186],\n",
            "          [0.4023, 0.5977]]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, 2, 2, 2)\n",
        "\n",
        "#Softmax occurs between batches. For instance, the\n",
        "#softmax for the value at the 1st pixel position in the 1st channel\n",
        "#of the 1st batch would be between the values at the 1st pixel position\n",
        "#in the 1st channel of all the 'N' batches.\n",
        "soft_1 = nn.Softmax(dim=0)\n",
        "\n",
        "#Softmax occurs between channels. For instance, the\n",
        "#softmax for the value at the 1st pixel position in the 1st channel\n",
        "#of some batch would be between the values at the 1st pixel position\n",
        "#in all the channels of this batch.\n",
        "soft_2 = nn.Softmax(dim=1)\n",
        "\n",
        "#Softmax occurs amongst the values along the height of each channel.\n",
        "soft_3 = nn.Softmax(dim=2)\n",
        "\n",
        "#Softmax occurs amongst the values along the width of each channel.\n",
        "soft_4 = nn.Softmax(dim=3)\n",
        "\n",
        "print(soft_1(x))\n",
        "print(soft_2(x))\n",
        "print(soft_3(x))\n",
        "print(soft_4(x))\n",
        "\n",
        "#Btw, the values that you use to softmax sum up to 1 after softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZFOzovSCcCs"
      },
      "source": [
        "Rough code to understand how the upsample class works in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "schwyXwi6bQa",
        "outputId": "7b73c4a0-7d2d-4b27-d692-7c2b2e070cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 447, 640])\n",
            "torch.Size([1, 3, 894, 1280])\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(requests.get(\"https://c8.alamy.com/zooms/9/7df650603bfe4193bab024ee29da5461/2btr9xg.jpg\", stream=True).raw)\n",
        "tensor_trans = transforms.ToTensor() #converting the image to a tensor\n",
        "img = tensor_trans(img) #converting the image to a tensor continued\n",
        "img = img.unsqueeze(0) #adding a batch dimension to the image.\n",
        "#Doing this because of the interpertation scheme of the shape values of tensors\n",
        "#by the Upsample class. If our image's shape was (x, y, z), Upsample would interpret\n",
        "#'x' as batch size, 'y' as number of channels and 'z' as the width. But obviously\n",
        "#'x' is number of channels, 'y' is the height and 'z' is the width. But if the shape\n",
        "#is (w, x, y, z), Upsample interprets 'w' as batch size, and 'x', 'y' and 'z' as wanted\n",
        "#above. Adding batch size is fine; it doesn't remove generality. If you only\n",
        "#want to upsample a single image, the batch size is just going to be 1. Btw, in\n",
        "#the (x, y, z) case, Upsample only scales the width -- both \"theoretically\" and\n",
        "#pratically, having a 3d tensor doesn't work for our purpose. For more clarification\n",
        "#and information: https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html\n",
        "\n",
        "print(img.shape)\n",
        "#by default, nearest is used. scale factor defines by what factor we want to increase\n",
        "#the height/width. new_height_width = old_height_width * scale_factor.\n",
        "upscale = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "img = upscale(img) #upsampling\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7erVYxI9fcg"
      },
      "source": [
        "Rough code to understand cropping in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZYugYeT9mly",
        "outputId": "3c516ae3-f77f-4641-93ce-927bde59573c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1, 512, 64, 64)\n",
        "#Look at the dotted lines in the paper Fig.1. I think that implies Center Crop.\n",
        "crop = transforms.CenterCrop(56)\n",
        "x = crop(x)\n",
        "print(x.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjZfm_7rHEOe"
      },
      "source": [
        "Rough code to understand how to do concat as required by the u-net paper in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hSV5VVnZVgt",
        "outputId": "a4ffb1cb-cbc4-47cf-a5be-36b13717a23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([2, 2, 4, 4])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 4, 4, 4])\n",
            "tensor([[[[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 2, 8, 4])\n",
            "tensor([[[[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "         [[1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0.]]]])\n",
            "torch.Size([1, 2, 4, 8])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(1, 2, 4, 4)\n",
        "y = torch.zeros(1, 2, 4, 4)\n",
        "\n",
        "#the dimension in torch.cat refer to the shape values above. Remember, shape values are\n",
        "#(batch_size, num_of_channels, height, width).\n",
        "\n",
        "#This does the concat on 'batch level' i.e. the new tensor will contain two image tensors x and y.\n",
        "print(torch.cat((x, y), 0))\n",
        "print(torch.cat((x, y), 0).shape)\n",
        "\n",
        "#This does the concat on 'channel level' i.e. the channels of y will be added to x\n",
        "#to create a new image tensor with more channels, specifically, new_number_of_\n",
        "#channels = no_of_channels_x + no_of_channels_y\n",
        "print(torch.cat((x, y), 1))\n",
        "print(torch.cat((x, y), 1).shape)\n",
        "\n",
        "#Concat on 'height level' i.e. rows of the 1st channel of y will be added to the rows\n",
        "#of the 1st channel of x vertically. This is done similarly for each channel.\n",
        "print(torch.cat((x,y), 2))\n",
        "print(torch.cat((x, y), 2).shape)\n",
        "\n",
        "#Concat on 'width level' i.e. rows of the 1st channel of y will be added to the rows\n",
        "#of the 1st channel of x horizontally. This is done similarly for each channel.\n",
        "print(torch.cat((x, y), 3))\n",
        "print(torch.cat((x, y), 3).shape)\n",
        "\n",
        "#Look at the outputs to understand more. It also really helps if you understand\n",
        "#how pytorch tensors are structured i.e. what each pair of [] represents -- specifically\n",
        "#in the context of images I guess in our case.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

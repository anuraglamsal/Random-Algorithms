{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFzxBYnJWf9cs+KJp0LPjX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraglamsal/Random-Algorithms/blob/main/XORnn_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a neural network model that can predict the XOR function in pytorch."
      ],
      "metadata": {
        "id": "YJ5-QOjpoFua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "TBAVoFKUlivi"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model class."
      ],
      "metadata": {
        "id": "9QVWaHK5mvlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XOR_model(torch.nn.Module): # Inherits from \"Module\"\n",
        "\n",
        "  def __init__(self): # We define the components of our architecture here.\n",
        "    super(XOR_model, self).__init__()\n",
        "    # Defining the hidden layer. Takes in an input vector with 2 values and has two neurons.\n",
        "    self.linear1 = torch.nn.Linear(2, 2)\n",
        "    # Defining the activation function that we'll be using.\n",
        "    # Here, we'll use sigmoid. A classic differentiable activation. Differentiability\n",
        "    # is important for backprop.\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "    # Defining our output layer. Takes in 2 values from\n",
        "    # the hidden layer and outputs, well, the output which is a single value ofc.\n",
        "    self.linear2 = torch.nn.Linear(2, 1)\n",
        "\n",
        "  def forward(self, x): # Defining the computations to be performed in a forward pass.\n",
        "    x = self.linear1(x) # Weighted sums of the inputs in the neurons of the hidden layer.\n",
        "    x = self.activation(x) # Apply activations to the outputs of the hidden neurons.\n",
        "    x = self.linear2(x) # Weighted sum in the output neuron.\n",
        "    x = self.activation(x) # Apply activation to the output of the output neuron to get the final output.\n",
        "    return x"
      ],
      "metadata": {
        "id": "sQdU10RKmndA"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our data should be tensors.\n",
        "* The params are floats and the output of the network is also going to contain float values, so make the inputs and target floats too.\n",
        "* momentum: https://paperswithcode.com/method/sgd-with-momentum"
      ],
      "metadata": {
        "id": "TXQn_Que4wZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "target = torch.tensor([0., 1., 1., 0.]).unsqueeze(1) # Adding an additional dimension to the tensor.\n",
        "                                                     # For some reason, the model output has an extra\n",
        "                                                     # dimension, and the target dimension and output dimension\n",
        "                                                     # should match in pytorch, therefore needing to \"unsqueeze.\"\n",
        "model = XOR_model() # You work with an object of the model class in order to get predictions, pass\n",
        "                    # model params to the optimizer, etc.\n",
        "loss_fn = torch.nn.BCELoss() # We'll be using the binary cross-entropy loss function.\n",
        "# Defining our optimizer (stochastic gradient descent) by passing the model params, defining learning rate, etc.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)"
      ],
      "metadata": {
        "id": "r8ol_Slvs3qc"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model."
      ],
      "metadata": {
        "id": "0EY2np8ishQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "  # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(inputs) # getting the model to predict.\n",
        "  loss = loss_fn(outputs, target) # calculating loss.\n",
        "  loss.backward() # Calculating the gradient. Gradient is simply a collection\n",
        "                  # of partial derivaties of the loss fn w.r.to each param. We'll\n",
        "                  # use these partial derivaties to change the param values such that we go towards minima.\n",
        "                  # Look up gradient descent to understand this stuff.\n",
        "  optimizer.step() # Performing the param updates.\n",
        "\n",
        "  if epoch % 1000 == 0: # Just printing out some losses.\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu4WvUGOWS4m",
        "outputId": "4ba5f40b-0d66-4fed-a0d0-1a550c293d7b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7037754058837891\n",
            "0.6876593232154846\n",
            "0.6181268692016602\n",
            "0.217415452003479\n",
            "0.08279238641262054\n",
            "0.04745560139417648\n",
            "0.03263457119464874\n",
            "0.024683739989995956\n",
            "0.019775405526161194\n",
            "0.016460904851555824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that around 10000 epochs is good enough to the desired result here. You can round the model's output to get the actual binary outputs."
      ],
      "metadata": {
        "id": "YyIxMUk-cXVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.round(model(inputs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E10TlC5KcWu8",
        "outputId": "bcbe8c2f-0b47-4c1b-93dd-e6f2cad9d1b5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]], grad_fn=<RoundBackward0>)\n"
          ]
        }
      ]
    }
  ]
}
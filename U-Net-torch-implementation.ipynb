{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch implementation of U-Net: https://arxiv.org/pdf/1505.04597.pdf <br><br>\n",
        "Note that the <span style=\"color:red\">training code</span> may have <span style=\"color:red\">errors</span> (Like the IOU calculation). But the U-Net impelementation part, I believe, is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-FGSWGKxDvF"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjxg3Os3iwIr"
      },
      "source": [
        "Fetching urls of training images and labels from github. https://github.com/anuraglamsal/Nepali-Currency-Images-and-Segmentation-Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxv0QdgcivpC"
      },
      "outputs": [],
      "source": [
        "def get_github_folder_contents(owner, repo, path):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(\"Failed to fetch folder contents.\")\n",
        "        return None\n",
        "\n",
        "def get_images(owner, repo, path):\n",
        "    contents = get_github_folder_contents(owner, repo, path)\n",
        "    images = [item['download_url'] for item in contents]\n",
        "    return images\n",
        "\n",
        "owner = \"anuraglamsal\"\n",
        "repo = \"training_images\"\n",
        "path_1 = \"images\"\n",
        "path_2 = \"labels\"\n",
        "\n",
        "images, labels = shuffle(get_images(owner, repo, path_1), get_images(owner, repo, path_2), random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UybW0OcNj2_8"
      },
      "source": [
        "Writing the dataset class and doing train, validation and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ugZhO_tj5CD"
      },
      "outputs": [],
      "source": [
        "class CurrencyDataset(Dataset):\n",
        "    def __init__(self, label_urls, image_urls, transform=None, target_transform=None):\n",
        "        self.label_urls = label_urls\n",
        "        self.image_urls = image_urls\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_urls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(requests.get(self.image_urls[idx], stream=True).raw)\n",
        "        label = Image.open(requests.get(self.label_urls[idx], stream=True).raw)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "split_idx_train = int(0.8 * len(labels))\n",
        "split_idx_test = split_idx_train + int(0.1 * len(labels))\n",
        "\n",
        "# transformations to do to the images\n",
        "transform = transforms.Compose([transforms.Resize((512, 512)), transforms.Grayscale(), transforms.ToTensor()])\n",
        "\n",
        "dataset_train = CurrencyDataset(labels[0:split_idx_train], images[0:split_idx_train], transform, transform)\n",
        "dataset_test = CurrencyDataset(labels[split_idx_train:split_idx_test], images[split_idx_train:split_idx_test], transform, transform)\n",
        "dataset_val = CurrencyDataset(labels[split_idx_test:], images[split_idx_train:], transform, transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdXTn25D4Bwo"
      },
      "source": [
        "* init_channel = number of channels in your input image.\n",
        "* next_channel = the number of channels that the result of the first convolution should have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdXOLEfw2OiF"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "  def __init__(self, init_channel, next_channel):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(init_channel, next_channel, 3, padding='same')\n",
        "    self.ReLU = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(next_channel, next_channel, 3, padding='same')\n",
        "    self.maxPool = nn.MaxPool2d(2, 2, ceil_mode=True)\n",
        "    self.conv3 = nn.Conv2d(next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv4 = nn.Conv2d(2 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv5 = nn.Conv2d(2 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv6 = nn.Conv2d(4 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv7 = nn.Conv2d(4 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv8 = nn.Conv2d(8 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv9 = nn.Conv2d(8 * next_channel, 16 * next_channel, 3, padding='same')\n",
        "    self.conv10 = nn.Conv2d(16 * next_channel, 16 * next_channel, 3, padding='same')\n",
        "    self.upsample = nn.Upsample(scale_factor=2)\n",
        "    self.conv11 = nn.Conv2d(16 * next_channel, 8 * next_channel, 2, padding='same')\n",
        "\n",
        "    #Apparently convtranspose2d also more or less does the same as upsample+conv2d.\n",
        "    #Would be interesting to explore this too perhaps. But the paper says\n",
        "    #upsample+conv, so I'll stick to this for now. Also, apparently, using transposed\n",
        "    #convolution could have \"checkboard artifacts\" as compared to upsample+conv:\n",
        "    #https://distill.pub/2016/deconv-checkerboard/\n",
        "\n",
        "    #self.conv11 = nn.ConvTranspose2d(16 * next_channel, 8 * next_channel, 2, 2)\n",
        "\n",
        "    self.conv12 = nn.Conv2d(16 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv13 = nn.Conv2d(8 * next_channel, 8 * next_channel, 3, padding='same')\n",
        "    self.conv14 = nn.Conv2d(8 * next_channel, 4 * next_channel, 2, padding='same')\n",
        "    self.conv15 = nn.Conv2d(8 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv16 = nn.Conv2d(4 * next_channel, 4 * next_channel, 3, padding='same')\n",
        "    self.conv17 = nn.Conv2d(4 * next_channel, 2 * next_channel, 2, padding='same')\n",
        "    self.conv18 = nn.Conv2d(4 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv19 = nn.Conv2d(2 * next_channel, 2 * next_channel, 3, padding='same')\n",
        "    self.conv20 = nn.Conv2d(2 * next_channel, next_channel, 2, padding='same')\n",
        "    self.conv21 = nn.Conv2d(2 * next_channel, next_channel, 3, padding='same')\n",
        "    self.conv22 = nn.Conv2d(next_channel, next_channel, 3, padding='same')\n",
        "    self.conv23 = nn.Conv2d(next_channel, 1, 1)\n",
        "\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    # self.initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    block_1 = self.ReLU(self.conv2(self.ReLU(self.conv1(x))))\n",
        "\n",
        "    block_2 = self.ReLU(self.conv4(self.ReLU(self.conv3(self.maxPool(block_1)))))\n",
        "\n",
        "    block_3 = self.ReLU(self.conv6(self.ReLU(self.conv5(self.maxPool(block_2)))))\n",
        "\n",
        "    block_4 = self.ReLU(self.conv8(self.ReLU(self.conv7(self.maxPool(block_3)))))\n",
        "    #print(block_4.shape[2])\n",
        "\n",
        "    block_5 = self.ReLU(self.conv10(self.ReLU(self.conv9(self.maxPool(block_4))))) # bottom-most block\n",
        "    #print(block_5.shape[2])\n",
        "\n",
        "    up_conv_1 = self.conv11(self.upsample(block_5))\n",
        "    crop_1 = transforms.CenterCrop(up_conv_1.shape[2])\n",
        "    block_6 = self.ReLU(self.conv13(self.ReLU(self.conv12(torch.cat((crop_1(block_4), up_conv_1), 1)))))\n",
        "    #print(block_6.shape[2])\n",
        "\n",
        "    up_conv_2 = self.conv14(self.upsample(block_6))\n",
        "    crop_2 = transforms.CenterCrop(up_conv_2.shape[2])\n",
        "    block_7 = self.ReLU(self.conv16(self.ReLU(self.conv15(torch.cat((crop_2(block_3), up_conv_2), 1)))))\n",
        "    #print(block_7.shape[2])\n",
        "\n",
        "    up_conv_3 = self.conv17(self.upsample(block_7))\n",
        "    crop_3 = transforms.CenterCrop(up_conv_3.shape[2])\n",
        "    block_8 = self.ReLU(self.conv19(self.ReLU(self.conv18(torch.cat((crop_3(block_2), up_conv_3), 1)))))\n",
        "    #print(block_8.shape[2])\n",
        "\n",
        "    up_conv_4 = self.conv20(self.upsample(block_8))\n",
        "    crop_4 = transforms.CenterCrop(up_conv_4.shape[2])\n",
        "    block_9 = self.conv23(self.ReLU(self.conv22(self.ReLU(self.conv21(torch.cat((crop_4(block_1), up_conv_4), 1))))))\n",
        "    #print(block_9.shape[2])\n",
        "\n",
        "    output = self.activation(block_9)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XuUPweJPysq"
      },
      "source": [
        "Weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRhuWsAHlnyC"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Apply Kaiming normal initialization\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    # Initialize the biases to zeros\n",
        "                    nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdVdCBgICAJM"
      },
      "source": [
        "Wandb init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "z5SbKpN_B_fd",
        "outputId": "725a7809-6c6c-462b-b882-7dfad822ab9f"
      },
      "outputs": [],
      "source": [
        "wandb.init( # for new runs\n",
        "    project=\"Currency Segmentation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hufQVCKfg0Cb"
      },
      "source": [
        "Training staging area. Can play with things here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zImZeW1RgzAb"
      },
      "outputs": [],
      "source": [
        "model = UNet(1, 64).to(torch.device('cuda'))\n",
        "loss_fn = nn.BCELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.99)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, min_lr=1e-6)\n",
        "\n",
        "initialize_weights(model)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=dataset_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=dataset_val,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=dataset_test,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "model_path = \"/path/to/model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZlTLGQzygV"
      },
      "source": [
        "Testing and validation code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDGqML24LWxG"
      },
      "outputs": [],
      "source": [
        "def test(loader, model):\n",
        "  model.eval()\n",
        "\n",
        "  total_ones_target = 0\n",
        "  total_ones_intersection = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for data, target in loader:\n",
        "      data, target = data.to(torch.device('cuda')), target.to(torch.device('cuda'))\n",
        "\n",
        "      output = model(data)\n",
        "\n",
        "      pred = (output > 0.5).float() # binarizing prediction\n",
        "      target = (target > 0.5).float() # binarizing actual\n",
        "\n",
        "      intersection = pred * target\n",
        "\n",
        "      total_ones_target += target.sum().item()\n",
        "      total_ones_intersection += intersection.sum().item()\n",
        "\n",
        "  accuracy = 100.0 * total_ones_intersection / total_ones_target\n",
        "\n",
        "  print(f\"Average IOU: {accuracy}\")\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NNdghybm8oN"
      },
      "source": [
        "Training code.\n",
        "\n",
        "Apparently padded convolutions is fine. The reason to not do it basically because of heavy computation. Look here: https://stackoverflow.com/questions/44014534/why-could-u-net-mask-image-with-smaller-mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsiUVNGDm8O2",
        "outputId": "dc381f73-c28d-404a-87b4-7aec890d586e"
      },
      "outputs": [],
      "source": [
        "_epoch = -1\n",
        "\n",
        "try:\n",
        "  checkpoint = torch.load(model_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  _epoch = checkpoint['epoch']\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.99)\n",
        "\n",
        "for epoch in range(_epoch+1, 100):\n",
        "  model.train()  # Set the model to training mode\n",
        "\n",
        "  #print(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "  total_epoch_loss = 0.0\n",
        "  total_num_of_pixels = 0\n",
        "\n",
        "  #print('\\n')\n",
        "\n",
        "  for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "     inputs, labels = inputs.to(torch.device('cuda')), labels.to(torch.device('cuda'))\n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     # Forward pass\n",
        "     outputs = model(inputs)\n",
        "     total_num_of_pixels += outputs.numel()\n",
        "\n",
        "     # Compute the loss\n",
        "     loss = loss_fn(outputs, labels)\n",
        "     total_epoch_loss += loss.item()\n",
        "\n",
        "     # Backward pass\n",
        "     loss.backward()\n",
        "\n",
        "     nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "     # Update the weights\n",
        "     optimizer.step()\n",
        "\n",
        "     #scheduler.step()\n",
        "     if batch_idx % 100 == 0:\n",
        "         print(total_epoch_loss / total_num_of_pixels)\n",
        "\n",
        "  #print('\\n')\n",
        "\n",
        "  avg_loss_per_pixel_train =  total_epoch_loss / total_num_of_pixels\n",
        "  acc = test(val_loader, model)\n",
        "\n",
        "  scheduler.step(int(acc)) # can play with this too i guess.\n",
        "\n",
        "  wandb.log({\"avg_loss_per_pixel_train\": avg_loss_per_pixel_train,\n",
        "               \"avg_IOU\": acc})\n",
        "\n",
        "  torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, model_path)\n",
        "\n",
        "print('\\n')\n",
        "print('Test set metrics: ')\n",
        "_ = test(test_loader, model)\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

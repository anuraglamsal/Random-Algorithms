{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3BdpoqCtUDxLYjF1tjDE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuraglamsal/Random-Algorithms/blob/main/XORnn_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "TBAVoFKUlivi"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model class."
      ],
      "metadata": {
        "id": "9QVWaHK5mvlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XOR_model(torch.nn.Module): # Inherits from \"Module\"\n",
        "\n",
        "  def __init__(self): # We define the components of our architecture here.\n",
        "    super(XOR_model, self).__init__()\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(2, 2) # Defining the hidden layer. Takes in an input vector with 2 values and has two neurons.\n",
        "    self.activation = torch.nn.Sigmoid() # Defining the activation function we'll be using. Here, we'll use sigmoid. A classic differentiable activation for backprop.\n",
        "    self.linear2 = torch.nn.Linear(2, 1) # Defining our output layer. Takes in 2 values from the hidden layer and outputs, well, the output which is a single value ofc.\n",
        "\n",
        "  def forward(self, x): # Defining the computations to be performed in a forward pass.\n",
        "    x = self.linear1(x) # Weighted sums of the inputs in the neurons of the hidden layer.\n",
        "    x = self.activation(x) # Apply activations to the outputs of the hidden neurons.\n",
        "    x = self.linear2(x) # Weighted sum in the output neuron.\n",
        "    x = self.activation(x) # Apply activation to the output of the output neuron to get the final output.\n",
        "    return x"
      ],
      "metadata": {
        "id": "sQdU10RKmndA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Our data should be tensors.\n",
        "* The params are floats and the output of the network is also going to contain float values, so make the inputs and target floats too.\n",
        "* momentum: https://paperswithcode.com/method/sgd-with-momentum"
      ],
      "metadata": {
        "id": "TXQn_Que4wZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "target = torch.tensor([0., 1., 1., 0.]).unsqueeze(1) # Adding an additional dimension to the tensor. For some reason, the model output has an extra dimension, and the target dimension and output dimension should match in pytorch, therefore needing to \"unsqueeze\".\n",
        "model = XOR_model() # You work with an object of the model class in order to get predictions, pass model params to the optimizer, etc.\n",
        "loss_fn = torch.nn.BCELoss() # We'll be using the binary cross-entropy loss function.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9) # Defining our optimizer (stochastic gradient descent) by passing the model params, defining learning rate, etc."
      ],
      "metadata": {
        "id": "r8ol_Slvs3qc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model."
      ],
      "metadata": {
        "id": "0EY2np8ishQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "  optimizer.zero_grad() # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "  outputs = model(inputs) # getting the model to predict.\n",
        "  loss = loss_fn(outputs, target) # calculating loss.\n",
        "  loss.backward() # Calculating the gradient. Gradient is simply a collection of partial derivaties of the loss fn w.r.to each param. We'll use these partial derivaties to change the param values such that we go towards minima. Look up gradient descent to understandt his stuff.\n",
        "  optimizer.step() # Performing the param updates.\n",
        "\n",
        "  if epoch % 1000 == 0: # Just printing out some losses.\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu4WvUGOWS4m",
        "outputId": "dc59f599-07c2-4f69-812b-7cc74523f078"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6934019327163696\n",
            "0.6880759596824646\n",
            "0.6436037421226501\n",
            "0.3797834515571594\n",
            "0.11400820314884186\n",
            "0.057247456163167953\n",
            "0.037153907120227814\n",
            "0.027235660701990128\n",
            "0.02140078693628311\n",
            "0.017581649124622345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that around 10000 epochs is good enough to the desired result here. You can round the model's output to get the actual binary outputs."
      ],
      "metadata": {
        "id": "YyIxMUk-cXVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.round(model(inputs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E10TlC5KcWu8",
        "outputId": "866a3522-11d7-4177-93eb-afce99ce13f2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]], grad_fn=<RoundBackward0>)\n"
          ]
        }
      ]
    }
  ]
}